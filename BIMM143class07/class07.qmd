---
title: "Class 7: Machine Learning 1"
author: "Lily Huynh (PID:A16929651)"
format: pdf
---

Today we will explore unsupervised machine learning methods including clustering and dimensionallity reduction methods. 

Let's start by making up some data (where we know there are clear groups) that we can use to test out different clustering methods. 

We can use the `rnorm()` function to help us here

```{r}
hist(rnorm(n=3000, mean=3))
```

Make data with two "clusters"

```{r}
x <- c( rnorm(30, mean = -3),
        rnorm(30, mean = +3))

z <- cbind(x=x, y=rev(x))
head(z)

plot(z)
```

How big is `z`

```{r}
nrow(z)
ncol(z)
```

## K-means clustering 

The main function in "base" R for K-means clustering is called `kmeans()`

```{r}
k <- kmeans(z, centers=2)
k
```

```{r}
attributes(k)
```
>Q. How many points lie in each cluster? 

```{r}
k$size
```

>Q. What component of our results tells us about the cluster membership (i.e. which point likes in which clusters)?

```{r}
k$cluster
```

>Q. Center of each cluster?

```{r}
k$centers
```

>Q. Put this result info together to make a little "base R" plot of our clustering result.  Also add the cluster center points to this plot. 

```{r}
plot(z, col="blue")
```

```{r}
plot(z, col=c("blue", "red"))
```

You can color by number
```{r}
plot(z, col=c(1,2))
```

Plot colored by cluster membership:

```{r}
plot(z, col=k$cluster)
points(k$centers, col="blue", pch=15)
```

>Q. Run kmeans on our input `z` and define 4 clusters, making the same results visualization plot as above (plot of z colored by cluster membership). 

```{r}
z
```


```{r}
k4 <- kmeans(z, centers = 4)
plot(z, col=k4$cluster)

```

## Hierarchical Clustering 

The main function in base R for this called `hclust()` it will take as input a distance matrix (key point is that you can't just five your "raw" data as input - you have to first calculate a distance matrix from your data).

```{r}
d <- dist(z)
hc <- hclust(d)
hc
```

```{r}
plot(hc)
abline(h=10, col="red")
```

Once I inspect the "tree", I can "cut" the tree to yield my groupings or clusters. The function to do this is called `cutree()`

```{r}
groups <- cutree(hc, h=10)
```

```{r}
plot(z, col=groups)
```

## Hands on with Principal Component Analysis (PCA)

Let's examine some silly 17-dimensional data detailing food consumption in the UK (England, Scotland, Wales, and N. Ireland). Are these countries eating habits different or similar and if so how?

### Data import

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
x
```

>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
nrow(x)
ncol(x)
dim(x)
```

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

>I prefer doing nrow() and ncol() because I don't have to remember if dim() gives me the number of rows first or number of columns first. I think nrow() and ncol() is more robust because we can chose a specific value without needing the $ sign. 

>Q3: Changing what optional argument in the above barplot() function results in the following plot?

>Changing the argument besides=T to beside=F changes it into a stacked graph. 

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

>Q5. Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

> The pairwise plots are showing different plots with different x and y axis. For example, N.Ireland can be on the x axis or the y axis. A given data point that lies on the diagonal that has a positive correlation means that the compared countries on the x and y axis are similar. 

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```

>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

>The main different between N. Ireland and the other countries of the UK is the amount of Fresh_potatoes, Alcoholic_drinks, and Fresh_fruit they eat. 

Looking at these types of "pairwise plots" can be helpful but it does not scale well and kind of sucks!
There must be a better way...

### PCA to the rescue!

The main function for PCA in base R is called `prcomp()`. This function wants the transpose of our input data - i.e. the important food in as columns and the countries as rows. 

```{r}
pca <- prcomp(t(x))
summary(pca)
```

>Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

>Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x),col= c("orange", "red", "blue", "green"))
```

Let's see what is in our PCA result object `pca`

```{r}
attributes(pca)
```

The `pca$x` result object is where we will focus on first, as this details how the countries are related to each other in terms of our new "axis" (aka. "PCs", "eigenvectors", etc.)

```{r}
head(pca$x)
```

```{r}
plot(pca$x[,1], pca$x[,2], pch=16,
     col= c("orange", "red", "blue", "darkgreen"),
     xlab="PC1", ylab="PC2")
```

We can look at the so-called PC "loadings" result object to see how the original foods contribute to our new PCs (i.e. how the original variables contribute to our new PC variables).

```{r}
pca$rotation[,1]
```

>Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

>Ans. The two prominantely food groups are fresh potatoes and soft drinks. PC2 mainly tells us that Fresh_potates is in the negative side of the plot and therefore pushes the countries down the plot. It also tells us that Soft_drinks is on the positive side of the plot and therefore pushes the countries to the top of the plot. 

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
z <- summary(pca)
z$importance
```


```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

```{r}
## Lets focus on PC2
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

>Q10: How many genes and samples are in this data set?

>Ans. There are 100 genes and 10 samples in this data set.

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

```{r}
# The genes are rows
nrow(rna.data)
# The samples are columns
ncol(rna.data)
```

```{r}
pca1 <- prcomp(t(rna.data),, scale=TRUE)

plot(pca1$x[,1], pca1$x[,2], xlab="PC1", ylab="PC2")
```

```{r}
summary(pca1)
```

```{r}
plot(pca1)
```

```{r}
pca1.var <- pca1$sdev^2

pca1.var.per <- round(pca1.var/sum(pca1.var)*100, 1)
pca1.var.per
```

```{r}
barplot(pca1.var.per, main="Scree Plot",
        names.arg = paste0("PC", 1:10),
        xlab="Principal Component", ylab="Percent Variation")
```

